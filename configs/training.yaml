# Supervised Fine-Tuning
sft:
  num_epochs: 3
  per_device_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0
  fp16: true
  logging_steps: 50
  eval_steps: 500
  save_steps: 1000
  output_dir: "models/sft"

# Reward Model Training
reward:
  num_epochs: 1
  per_device_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-5
  warmup_steps: 100
  output_dir: "models/reward"

# PPO Training
ppo:
  total_steps: 5000
  batch_size: 8
  mini_batch_size: 2
  learning_rate: 1.5e-5
  ppo_epochs: 4
  kl_coef: 0.2
  clip_range: 0.2
  vf_coef: 0.1
  gamma: 1.0
  lam: 0.95
  max_new_tokens: 150
  temperature: 0.9
  output_dir: "models/rlhf"