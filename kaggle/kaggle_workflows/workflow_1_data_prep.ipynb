{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow 1: Data Preparation\n",
    "\n",
    "This notebook outlines the steps for preparing the datasets required for training the persona-consistent chatbot. It includes data loading, preprocessing, and validation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths to raw data\n",
    "raw_data_path = '../data/raw/'\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# Load datasets\n",
    "def load_datasets():\n",
    "    personachat = pd.read_json(os.path.join(raw_data_path, 'personachat', 'data.json'))\n",
    "    blended_skill_talk = pd.read_json(os.path.join(raw_data_path, 'blended_skill_talk', 'data.json'))\n",
    "    custom_personas = pd.read_json(os.path.join(raw_data_path, 'custom_personas', 'data.json'))\n",
    "    return personachat, blended_skill_talk, custom_personas\n",
    "\n",
    "personachat, blended_skill_talk, custom_personas = load_datasets()"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_preprocessing"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing function\n",
    "def preprocess_data(df):\n",
    "    # Example preprocessing steps\n",
    "    df = df.dropna()  # Remove missing values\n",
    "    df['text'] = df['text'].str.lower()  # Convert text to lowercase\n",
    "    return df\n",
    "\n",
    "# Preprocess datasets\n",
    "personachat_cleaned = preprocess_data(personachat)\n",
    "blended_skill_talk_cleaned = preprocess_data(blended_skill_talk)\n",
    "custom_personas_cleaned = preprocess_data(custom_personas)"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_processed_data"
   },
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "def save_processed_data(df, filename):\n",
    "    df.to_json(os.path.join(processed_data_path, filename), orient='records', lines=True)\n",
    "\n",
    "save_processed_data(personachat_cleaned, 'sft/train.jsonl')\n",
    "save_processed_data(blended_skill_talk_cleaned, 'sft/val.jsonl')\n",
    "save_processed_data(custom_personas_cleaned, 'sft/test.jsonl')"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this workflow, we successfully loaded, preprocessed, and saved the datasets required for training the persona-consistent chatbot. The next steps will involve training the model using these datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}