{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Evaluation Suite\n\n**Target:** 85%+ persona consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft wandb evaluate matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import torch, json, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data.loader import DatasetLoader\n",
    "from src.data.processor import DataProcessor\n",
    "from src.model.base import load_base_model, load_tokenizer\n",
    "from src.eval.persona import PersonaEvaluator\n",
    "from src.eval.engagement import EngagementEvaluator\n",
    "from src.eval.quality import QualityEvaluator\n",
    "from src.eval.benchmark import BenchmarkEvaluator\n",
    "\n",
    "# Optional wandb - no API key required\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(project='persona-chatbot-rlhf', name='evaluation', mode='disabled')  # offline mode\n",
    "    USE_WANDB = True\n",
    "except:\n",
    "    USE_WANDB = False\n",
    "    class wandb:\n",
    "        @staticmethod\n",
    "        def log(*args, **kwargs): pass\n",
    "        @staticmethod\n",
    "        def finish(): pass\n",
    "        class Image:\n",
    "            def __init__(self, *args, **kwargs): pass\n",
    "\n",
    "print(f'W&B: {\"enabled (offline)\" if USE_WANDB else \"disabled\"}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\n",
    "models = {\n",
    "    'baseline': load_base_model({'name': 'gpt2-medium', 'device_map': 'auto'}),\n",
    "    'sft': load_base_model({'name': '../models/sft/final', 'device_map': 'auto'}),\n",
    "    'rlhf': load_base_model({'name': '../models/rlhf/checkpoint-final', 'device_map': 'auto'})\n",
    "}\n",
    "tokenizer = load_tokenizer({'name': 'gpt2-medium'})\n",
    "print('\u2705 Models loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DatasetLoader()\n",
    "test_data = loader.load_personachat(split='validation', use_synthetic=True)\n",
    "processor = DataProcessor(config={'base_model': 'gpt2-medium', 'max_length': 512})\n",
    "processed_test = processor.preprocess(test_data)\n",
    "print(f'Test data: {len(processed_test)} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persona Consistency (TARGET: 85%+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_eval = PersonaEvaluator(tokenizer_name='gpt2-medium')\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'Evaluating {name}...')\n",
    "    consistency = persona_eval.evaluate_consistency(model, processed_test, max_samples=200, generate_responses=True)\n",
    "    results[name] = {'persona_consistency': consistency}\n",
    "    print(f'  {name}: {consistency:.3f}')\n",
    "    wandb.log({f'{name}_persona_consistency': consistency})\n",
    "\n",
    "# Check RLHF target\n",
    "rlhf_consistency = results['rlhf']['persona_consistency']\n",
    "target_met = rlhf_consistency >= 0.85\n",
    "print(f'\\nRLHF Persona Consistency: {rlhf_consistency:.3f}')\n",
    "print(f'Target (85%): {\"\u2705 ACHIEVED\" if target_met else \"\u274c NOT MET\"}')\n",
    "wandb.log({'rlhf_target_85%_met': target_met})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engagement & Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_eval = EngagementEvaluator()\n",
    "quality_eval = QualityEvaluator()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\nEvaluating {name}...')\n",
    "    \n",
    "    # Engagement\n",
    "    engagement = engagement_eval.evaluate_engagement(model, processed_test, max_samples=200, generate_responses=True)\n",
    "    results[name]['engagement'] = engagement\n",
    "    \n",
    "    # Quality\n",
    "    perplexity = quality_eval.compute_perplexity(model, processed_test, text_field='text', batch_size=8)\n",
    "    bleu = quality_eval.compute_bleu(model, processed_test, max_samples=100)\n",
    "    rouge = quality_eval.compute_rouge(model, processed_test, max_samples=100)\n",
    "    \n",
    "    results[name].update({'perplexity': perplexity, 'bleu': bleu['bleu'], 'rouge1': rouge['rouge1']})\n",
    "    \n",
    "    print(f'  Engagement: {engagement:.3f}, Perplexity: {perplexity:.2f}, BLEU: {bleu[\"bleu\"]:.3f}')\n",
    "    \n",
    "    wandb.log({f'{name}_engagement': engagement, f'{name}_perplexity': perplexity, f'{name}_bleu': bleu['bleu']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import os\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "df = pd.DataFrame(results).T\n",
    "print('\\nComparison:')\n",
    "print(df.to_string())\n",
    "df.to_csv('../outputs/evaluation_results.csv')\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "df['persona_consistency'].plot(kind='bar', ax=axes[0], title='Persona Consistency')\n",
    "axes[0].axhline(y=0.85, color='r', linestyle='--', label='Target')\n",
    "axes[0].legend()\n",
    "df['engagement'].plot(kind='bar', ax=axes[1], title='Engagement', color='orange')\n",
    "df['bleu'].plot(kind='bar', ax=axes[2], title='BLEU', color='green')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/evaluation_comparison.png', dpi=300)\n",
    "if USE_WANDB:\n",
    "    wandb.log({'comparison': wandb.Image(plt)})\n",
    "plt.show()\n",
    "print('\u2705 Saved to outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "print('\u2705 Complete! Next: 6_analysis_demo.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}