{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Evaluation Suite\n\n**Target:** 85%+ persona consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft wandb evaluate matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\nimport torch, wandb, json, pandas as pd\nimport matplotlib.pyplot as plt\nfrom src.data.loader import DatasetLoader\nfrom src.data.processor import DataProcessor\nfrom src.model.base import load_base_model, load_tokenizer\nfrom src.eval.persona import PersonaEvaluator\nfrom src.eval.engagement import EngagementEvaluator\nfrom src.eval.quality import QualityEvaluator\nfrom src.eval.benchmark import BenchmarkEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='persona-chatbot-rlhf', name='evaluation', tags=['eval'])\nprint(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\nmodels = {\n    'baseline': load_base_model({'name': 'gpt2-medium', 'device_map': 'auto'}),\n    'sft': load_base_model({'name': '../models/sft/final', 'device_map': 'auto'}),\n    'rlhf': load_base_model({'name': '../models/rlhf/checkpoint-final', 'device_map': 'auto'})\n}\ntokenizer = load_tokenizer({'name': 'gpt2-medium'})\nprint('\u2705 Models loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DatasetLoader()\ntest_data = loader.load_personachat(split='validation', use_synthetic=True)\nprocessor = DataProcessor(config={'base_model': 'gpt2-medium', 'max_length': 512})\nprocessed_test = processor.preprocess(test_data)\nprint(f'Test data: {len(processed_test)} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persona Consistency (TARGET: 85%+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_eval = PersonaEvaluator(tokenizer_name='gpt2-medium')\nresults = {}\n\nfor name, model in models.items():\n    print(f'Evaluating {name}...')\n    consistency = persona_eval.evaluate_consistency(model, processed_test, max_samples=200, generate_responses=True)\n    results[name] = {'persona_consistency': consistency}\n    print(f'  {name}: {consistency:.3f}')\n    wandb.log({f'{name}_persona_consistency': consistency})\n\n# Check RLHF target\nrlhf_consistency = results['rlhf']['persona_consistency']\ntarget_met = rlhf_consistency >= 0.85\nprint(f'\\nRLHF Persona Consistency: {rlhf_consistency:.3f}')\nprint(f'Target (85%): {\"\u2705 ACHIEVED\" if target_met else \"\u274c NOT MET\"}')\nwandb.log({'rlhf_target_85%_met': target_met})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engagement & Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engagement_eval = EngagementEvaluator()\nquality_eval = QualityEvaluator()\n\nfor name, model in models.items():\n    print(f'\\nEvaluating {name}...')\n    \n    # Engagement\n    engagement = engagement_eval.evaluate_engagement(model, processed_test, max_samples=200, generate_responses=True)\n    results[name]['engagement'] = engagement\n    \n    # Quality\n    perplexity = quality_eval.compute_perplexity(model, processed_test, text_field='text', batch_size=8)\n    bleu = quality_eval.compute_bleu(model, processed_test, max_samples=100)\n    rouge = quality_eval.compute_rouge(model, processed_test, max_samples=100)\n    \n    results[name].update({'perplexity': perplexity, 'bleu': bleu['bleu'], 'rouge1': rouge['rouge1']})\n    \n    print(f'  Engagement: {engagement:.3f}, Perplexity: {perplexity:.2f}, BLEU: {bleu[\"bleu\"]:.3f}')\n    \n    wandb.log({f'{name}_engagement': engagement, f'{name}_perplexity': perplexity, f'{name}_bleu': bleu['bleu']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\ndf = pd.DataFrame(results).T\nprint('\\nComparison:')\nprint(df.to_string())\ndf.to_csv('../outputs/evaluation_results.csv')\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\ndf['persona_consistency'].plot(kind='bar', ax=axes[0], title='Persona Consistency')\naxes[0].axhline(y=0.85, color='r', linestyle='--', label='Target')\naxes[0].legend()\ndf['engagement'].plot(kind='bar', ax=axes[1], title='Engagement', color='orange')\ndf['bleu'].plot(kind='bar', ax=axes[2], title='BLEU', color='green')\nplt.tight_layout()\nplt.savefig('../outputs/evaluation_comparison.png', dpi=300)\nwandb.log({'comparison': wandb.Image(plt)})\nplt.show()\nprint('\u2705 Saved to outputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\nprint('\u2705 Complete! Next: 6_analysis_demo.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}