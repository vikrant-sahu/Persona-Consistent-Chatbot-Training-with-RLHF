{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model & PPO Training\n\n**RLHF Phase:** Train reward model and optimize with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft trl accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\nimport torch, wandb, json, os\nfrom src.data.loader import DatasetLoader\nfrom src.data.processor import DataProcessor\nfrom src.data.generator import PreferenceGenerator\nfrom src.model.base import load_base_model, load_tokenizer\nfrom src.model.reward import RewardModel\nfrom src.training.reward import RewardModelTrainer\nfrom src.training.ppo import PPOTrainer\nfrom src.utils.metrics import MetricsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\nreward_config = {'num_labels': 1, 'lora_r': 8, 'lora_alpha': 16}\nreward_train_config = {'output_dir': '../models/reward', 'num_epochs': 1, 'per_device_batch_size': 2, 'gradient_accumulation_steps': 8, 'learning_rate': 1e-5, 'warmup_steps': 100}\nppo_config = {'model_name': 'gpt2-medium', 'tokenizer_name': 'gpt2-medium', 'output_dir': '../models/rlhf', 'total_steps': 5000, 'batch_size': 8, 'learning_rate': 1.5e-5, 'ppo_epochs': 4, 'kl_coef': 0.2, 'clip_range': 0.2, 'vf_coef': 0.1, 'gamma': 1.0, 'lam': 0.95, 'max_new_tokens': 150, 'temperature': 0.9, 'save_freq': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='persona-chatbot-rlhf', name='reward-ppo', config={**reward_config, **ppo_config}, tags=['reward', 'ppo'])\nprint(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SFT model from notebook 3\nsft_path = '../models/sft/final'\nmodel = load_base_model({'name': sft_path, 'device_map': 'auto'})\ntokenizer = load_tokenizer({'name': sft_path})\nprint(f'\u2705 SFT model loaded from {sft_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and generate preference pairs\nloader = DatasetLoader()\ntrain = loader.load_personachat(split='train', use_synthetic=True)\nprocessor = DataProcessor(config={'base_model': 'gpt2-medium', 'max_length': 512})\nprocessed = processor.preprocess(train[:5000])  # Use subset\ngenerator = PreferenceGenerator(config={})\npairs = generator.generate_pairs(processed, model=model)\nprint(f'Generated {len(pairs)} preference pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train reward model\nreward_model = RewardModel(base_model='gpt2-medium', config=reward_config)\nprint('Training reward model...')\nreward_trainer = RewardModelTrainer(reward_model, pairs[:4000], pairs[4000:], reward_train_config)\nreward_results = reward_trainer.train()\nreward_model.save(f\"{reward_train_config['output_dir']}/final\")\nprint(f'\u2705 Reward model trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prompts and create reference model\nprompts = generator.extract_prompts(processed)\nprint(f'Prepared {len(prompts)} prompts for PPO')\n\n# Create frozen reference model\nref_model = load_base_model({'name': sft_path, 'device_map': 'auto'})\nref_model.eval()\nfor param in ref_model.parameters():\n    param.requires_grad = False\nprint('\u2705 Reference model created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO trainer (PASS TOKENIZER!)\nos.makedirs(ppo_config['output_dir'], exist_ok=True)\nppo_trainer = PPOTrainer(policy_model=model, reward_model=reward_model, ref_model=ref_model, config=ppo_config, tokenizer=tokenizer)\nprint('\ud83d\ude80 Starting PPO training...')\nppo_results = ppo_trainer.train(prompts)\nprint(f'\u2705 PPO complete! Final reward: {ppo_results[\"final_reward\"]:.4f}')\nwandb.log({'final_ppo_reward': ppo_results['final_reward']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\nprint('\u2705 Complete! Next: 5_evaluation.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}