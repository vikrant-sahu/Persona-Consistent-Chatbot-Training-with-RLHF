{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model & PPO Training\n\n**RLHF Phase:** Train reward model and optimize with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft trl accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import torch, json, os\n",
    "from src.data.loader import DatasetLoader\n",
    "from src.data.processor import DataProcessor\n",
    "from src.data.generator import PreferenceGenerator\n",
    "from src.model.base import load_base_model, load_tokenizer\n",
    "from src.model.reward import RewardModel\n",
    "from src.training.reward import RewardModelTrainer\n",
    "from src.training.ppo import PPOTrainer\n",
    "from src.utils.metrics import MetricsTracker\n",
    "\n",
    "# Optional wandb - no API key required\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(project='persona-chatbot-rlhf', name='reward-ppo', mode='disabled')  # offline mode\n",
    "    USE_WANDB = True\n",
    "except:\n",
    "    USE_WANDB = False\n",
    "    class wandb:\n",
    "        @staticmethod\n",
    "        def log(*args, **kwargs): pass\n",
    "        @staticmethod\n",
    "        def finish(): pass\n",
    "\n",
    "print(f'W&B: {\"enabled (offline)\" if USE_WANDB else \"disabled\"}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "reward_config = {'num_labels': 1, 'lora_r': 8, 'lora_alpha': 16}\n",
    "reward_train_config = {'output_dir': '../models/reward', 'num_epochs': 1, 'per_device_batch_size': 2, 'gradient_accumulation_steps': 8, 'learning_rate': 1e-5, 'warmup_steps': 100}\n",
    "ppo_config = {'model_name': 'gpt2-medium', 'tokenizer_name': 'gpt2-medium', 'output_dir': '../models/rlhf', 'total_steps': 5000, 'batch_size': 8, 'learning_rate': 1.5e-5, 'ppo_epochs': 4, 'kl_coef': 0.2, 'clip_range': 0.2, 'vf_coef': 0.1, 'gamma': 1.0, 'lam': 0.95, 'max_new_tokens': 150, 'temperature': 0.9, 'save_freq': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SFT model from notebook 3\n",
    "sft_path = '../models/sft/final'\n",
    "model = load_base_model({'name': sft_path, 'device_map': 'auto'})\n",
    "tokenizer = load_tokenizer({'name': sft_path})\n",
    "print(f'\u2705 SFT model loaded from {sft_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and generate preference pairs\n",
    "loader = DatasetLoader()\n",
    "train = loader.load_personachat(split='train', use_synthetic=True)\n",
    "processor = DataProcessor(config={'base_model': 'gpt2-medium', 'max_length': 512})\n",
    "processed = processor.preprocess(train[:5000])  # Use subset\n",
    "generator = PreferenceGenerator(config={})\n",
    "pairs = generator.generate_pairs(processed, model=model)\n",
    "print(f'Generated {len(pairs)} preference pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train reward model\n",
    "reward_model = RewardModel(base_model='gpt2-medium', config=reward_config)\n",
    "print('Training reward model...')\n",
    "reward_trainer = RewardModelTrainer(reward_model, pairs[:4000], pairs[4000:], reward_train_config)\n",
    "reward_results = reward_trainer.train()\n",
    "reward_model.save(f\"{reward_train_config['output_dir']}/final\")\n",
    "print(f'\u2705 Reward model trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prompts and create reference model\n",
    "prompts = generator.extract_prompts(processed)\n",
    "print(f'Prepared {len(prompts)} prompts for PPO')\n",
    "\n",
    "# Create frozen reference model\n",
    "ref_model = load_base_model({'name': sft_path, 'device_map': 'auto'})\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print('\u2705 Reference model created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO trainer (PASS TOKENIZER!)\n",
    "os.makedirs(ppo_config['output_dir'], exist_ok=True)\n",
    "ppo_trainer = PPOTrainer(policy_model=model, reward_model=reward_model, ref_model=ref_model, config=ppo_config, tokenizer=tokenizer)\n",
    "print('\ud83d\ude80 Starting PPO training...')\n",
    "ppo_results = ppo_trainer.train(prompts)\n",
    "print(f'\u2705 PPO complete! Final reward: {ppo_results[\"final_reward\"]:.4f}')\n",
    "wandb.log({'final_ppo_reward': ppo_results['final_reward']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "print('\u2705 Complete! Next: 5_evaluation.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}