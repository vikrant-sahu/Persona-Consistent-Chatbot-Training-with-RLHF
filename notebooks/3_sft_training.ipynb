{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT with LoRA\n\n**Goals:** 75-80% cost reduction, 60-70% time reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft trl accelerate wandb evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import torch, time, json, os\n",
    "from src.data.loader import DatasetLoader\n",
    "from src.data.processor import DataProcessor\n",
    "from src.model.base import load_base_model, load_tokenizer, get_model_info\n",
    "from src.model.lora import LoRAWrapper\n",
    "from src.training.sft import SFTTrainer\n",
    "from src.utils.metrics import MetricsTracker\n",
    "\n",
    "# Optional wandb - no API key required\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(project='persona-chatbot-rlhf', name='sft-lora', mode='disabled')  # offline mode\n",
    "    USE_WANDB = True\n",
    "except:\n",
    "    USE_WANDB = False\n",
    "    class wandb:\n",
    "        @staticmethod\n",
    "        def log(*args, **kwargs): pass\n",
    "        @staticmethod\n",
    "        def finish(): pass\n",
    "\n",
    "print(f'W&B: {\"enabled (offline)\" if USE_WANDB else \"disabled\"}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\nmodel_config = {'name': 'gpt2-medium', 'cache_dir': '../models/base', 'device_map': 'auto'}\nlora_config = {'r': 16, 'alpha': 32, 'dropout': 0.1, 'target_modules': ['c_attn', 'c_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM'}\ntraining_config = {'output_dir': '../models/sft', 'num_epochs': 3, 'per_device_batch_size': 4, 'gradient_accumulation_steps': 4, 'learning_rate': 2e-4, 'warmup_steps': 500, 'weight_decay': 0.01, 'logging_steps': 50, 'eval_steps': 500, 'save_steps': 1000, 'use_wandb': False}\ndata_config = {'base_model': 'gpt2-medium', 'max_length': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DatasetLoader()\n",
    "train = loader.load_personachat(split='train', use_synthetic=True)\n",
    "val = loader.load_personachat(split='validation', use_synthetic=True)\n",
    "processor = DataProcessor(config=data_config)\n",
    "train_proc = processor.tokenize(processor.preprocess(train))\n",
    "val_proc = processor.tokenize(processor.preprocess(val))\n",
    "print(f'Data: {len(train_proc)} train, {len(val_proc)} val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model + LoRA\n",
    "model = load_base_model(model_config)\n",
    "tokenizer = load_tokenizer(model_config)\n",
    "total_p = get_model_info(model)['total_parameters']\n",
    "lora = LoRAWrapper(lora_config)\n",
    "model = lora.apply_lora(model, lora_config)\n",
    "lora.print_trainable_params(model)\n",
    "trainable_p = get_model_info(model)['trainable_parameters']\n",
    "reduction = (1 - trainable_p/total_p) * 100\n",
    "print(f'Params: {total_p/1e6:.0f}M -> {trainable_p/1e6:.1f}M ({reduction:.1f}% reduction)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "tracker = MetricsTracker(gpu_hourly_rate=0.35)\n",
    "tracker.start_timing()\n",
    "os.makedirs(training_config['output_dir'], exist_ok=True)\n",
    "trainer = SFTTrainer(model, tokenizer, train_proc, val_proc, training_config)\n",
    "print('Training...')\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "hours = tracker.stop_timing() / 3600\n",
    "gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "cost = tracker.track_cost(hours, gpus)\n",
    "savings = tracker.calculate_savings('full_finetuning', 'lora')\n",
    "print(f'Time: {hours:.2f}h, Cost: ${cost:.2f}')\n",
    "print(f'Savings: Time {savings[\"time_savings_percent\"]:.1f}% (target 60-70%), Cost {savings[\"cost_savings_percent\"]:.1f}% (target 75-80%)')\n",
    "cost_ok = savings['cost_savings_percent'] >= 75\n",
    "time_ok = savings['time_savings_percent'] >= 60\n",
    "print(f'Targets: Cost {\"\u2705\" if cost_ok else \"\u274c\"}, Time {\"\u2705\" if time_ok else \"\u274c\"}')\n",
    "wandb.log({'hours': hours, 'cost': cost, 'time_savings%': savings['time_savings_percent'], 'cost_savings%': savings['cost_savings_percent']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "path = f\"{training_config['output_dir']}/final\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)\n",
    "with open(f\"{training_config['output_dir']}/summary.json\", 'w') as f:\n",
    "    json.dump({'params': int(trainable_p), 'hours': hours, 'cost': cost, 'savings_time%': savings['time_savings_percent'], 'savings_cost%': savings['cost_savings_percent'], 'targets_met': {'cost': cost_ok, 'time': time_ok}}, f, indent=2)\n",
    "wandb.finish()\n",
    "print(f'\u2705 Saved to {path}. Next: 4_reward_and_ppo.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}