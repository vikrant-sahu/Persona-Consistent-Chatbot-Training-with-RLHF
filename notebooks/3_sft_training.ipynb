{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT with LoRA\n\n**Goals:** 75-80% cost reduction, 60-70% time reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft trl accelerate wandb evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\nimport torch, wandb, time, json, os\nfrom src.data.loader import DatasetLoader\nfrom src.data.processor import DataProcessor\nfrom src.model.base import load_base_model, load_tokenizer, get_model_info\nfrom src.model.lora import LoRAWrapper\nfrom src.training.sft import SFTTrainer\nfrom src.utils.metrics import MetricsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\nmodel_config = {'name': 'gpt2-medium', 'cache_dir': '../models/base', 'device_map': 'auto', 'fp16': True}\nlora_config = {'r': 16, 'alpha': 32, 'dropout': 0.1, 'target_modules': ['c_attn', 'c_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM'}\ntraining_config = {'output_dir': '../models/sft', 'num_epochs': 3, 'per_device_batch_size': 4, 'gradient_accumulation_steps': 4, 'learning_rate': 2e-4, 'warmup_steps': 500, 'weight_decay': 0.01, 'fp16': True, 'logging_steps': 50, 'eval_steps': 500, 'save_steps': 1000, 'use_wandb': True}\ndata_config = {'base_model': 'gpt2-medium', 'max_length': 512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='persona-chatbot-rlhf', name='sft-lora', config={**model_config, **lora_config, **training_config}, tags=['sft'])\nprint(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\nloader = DatasetLoader()\ntrain = loader.load_personachat(split='train', use_synthetic=True)\nval = loader.load_personachat(split='validation', use_synthetic=True)\nprocessor = DataProcessor(config=data_config)\ntrain_proc = processor.tokenize(processor.preprocess(train))\nval_proc = processor.tokenize(processor.preprocess(val))\nprint(f'Data: {len(train_proc)} train, {len(val_proc)} val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model + LoRA\nmodel = load_base_model(model_config)\ntokenizer = load_tokenizer(model_config)\ntotal_p = get_model_info(model)['total_parameters']\nlora = LoRAWrapper(lora_config)\nmodel = lora.apply_lora(model, lora_config)\nlora.print_trainable_params(model)\ntrainable_p = get_model_info(model)['trainable_parameters']\nreduction = (1 - trainable_p/total_p) * 100\nprint(f'Params: {total_p/1e6:.0f}M -> {trainable_p/1e6:.1f}M ({reduction:.1f}% reduction)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\ntracker = MetricsTracker(gpu_hourly_rate=0.35)\ntracker.start_timing()\nos.makedirs(training_config['output_dir'], exist_ok=True)\ntrainer = SFTTrainer(model, tokenizer, train_proc, val_proc, training_config)\nprint('Training...')\nresults = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\nhours = tracker.stop_timing() / 3600\ngpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\ncost = tracker.track_cost(hours, gpus)\nsavings = tracker.calculate_savings('full_finetuning', 'lora')\nprint(f'Time: {hours:.2f}h, Cost: ${cost:.2f}')\nprint(f'Savings: Time {savings[\"time_savings_percent\"]:.1f}% (target 60-70%), Cost {savings[\"cost_savings_percent\"]:.1f}% (target 75-80%)')\ncost_ok = savings['cost_savings_percent'] >= 75\ntime_ok = savings['time_savings_percent'] >= 60\nprint(f'Targets: Cost {\"\u2705\" if cost_ok else \"\u274c\"}, Time {\"\u2705\" if time_ok else \"\u274c\"}')\nwandb.log({'hours': hours, 'cost': cost, 'time_savings%': savings['time_savings_percent'], 'cost_savings%': savings['cost_savings_percent']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\npath = f\"{training_config['output_dir']}/final\"\nos.makedirs(path, exist_ok=True)\nmodel.save_pretrained(path)\ntokenizer.save_pretrained(path)\nwith open(f\"{training_config['output_dir']}/summary.json\", 'w') as f:\n    json.dump({'params': int(trainable_p), 'hours': hours, 'cost': cost, 'savings_time%': savings['time_savings_percent'], 'savings_cost%': savings['cost_savings_percent'], 'targets_met': {'cost': cost_ok, 'time': time_ok}}, f, indent=2)\nwandb.finish()\nprint(f'\u2705 Saved to {path}. Next: 4_reward_and_ppo.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}