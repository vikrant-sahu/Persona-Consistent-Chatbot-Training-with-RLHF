{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis & Interactive Demo\n\n**Final verification of all project goals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft wandb matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\nimport torch, wandb, json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom src.model.base import load_base_model, load_tokenizer\nfrom src.utils.metrics import MetricsTracker\nfrom src.eval.persona import PersonaEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='persona-chatbot-rlhf', name='analysis-demo', tags=['demo'])\nprint(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Best Model (RLHF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_base_model({'name': '../models/rlhf/checkpoint-final', 'device_map': 'auto'})\ntokenizer = load_tokenizer({'name': '../models/rlhf/checkpoint-final'})\nmodel.eval()\nprint('\u2705 RLHF model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify All Project Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training summary\nwith open('../models/sft/summary.json', 'r') as f:\n    sft_summary = json.load(f)\n\n# Load evaluation results\nimport pandas as pd\neval_results = pd.read_csv('../outputs/evaluation_results.csv', index_col=0)\n\nprint('\ud83c\udfaf PROJECT GOALS VERIFICATION')\nprint('=' * 60)\n\n# Goal 1: Cost Reduction (75-80%)\ncost_savings = sft_summary['savings']['cost_%']\ncost_met = cost_savings >= 75\nprint(f'1. Cost Reduction:')\nprint(f'   Target: 75-80%')\nprint(f'   Achieved: {cost_savings:.1f}%')\nprint(f'   Status: {\"\u2705 ACHIEVED\" if cost_met else \"\u274c NOT MET\"}')\n\n# Goal 2: Time Reduction (60-70%)\ntime_savings = sft_summary['savings']['time_%']\ntime_met = time_savings >= 60\nprint(f'\\n2. Time Reduction:')\nprint(f'   Target: 60-70%')\nprint(f'   Achieved: {time_savings:.1f}%')\nprint(f'   Status: {\"\u2705 ACHIEVED\" if time_met else \"\u274c NOT MET\"}')\n\n# Goal 3: Persona Consistency (85%+)\npersona_consistency = eval_results.loc['rlhf', 'persona_consistency']\npersona_met = persona_consistency >= 0.85\nprint(f'\\n3. Persona Consistency:')\nprint(f'   Target: 85%+')\nprint(f'   Achieved: {persona_consistency:.1%}')\nprint(f'   Status: {\"\u2705 ACHIEVED\" if persona_met else \"\u274c NOT MET\"}')\n\n# Overall\nall_met = cost_met and time_met and persona_met\nprint(f'\\n{\"=\" * 60}')\nprint(f'OVERALL: {\"\u2705 ALL GOALS ACHIEVED\" if all_met else \"\u26a0\ufe0f SOME GOALS NOT MET\"}')\nprint(f'{\"=\" * 60}')\n\nwandb.log({'goal_cost_met': cost_met, 'goal_time_met': time_met, 'goal_persona_met': persona_met, 'all_goals_met': all_met})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost-Benefit Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize savings\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Cost & Time Savings\nsavings_data = {'Cost Savings': cost_savings, 'Time Savings': time_savings}\naxes[0].bar(savings_data.keys(), savings_data.values(), color=['green', 'blue'])\naxes[0].axhline(y=75, color='r', linestyle='--', label='Cost Target (75%)')\naxes[0].axhline(y=60, color='orange', linestyle='--', label='Time Target (60%)')\naxes[0].set_ylabel('Savings (%)')\naxes[0].set_title('LoRA Efficiency Gains')\naxes[0].legend()\naxes[0].set_ylim(0, 100)\n\n# Model Comparison\nmodels = ['Baseline', 'SFT', 'RLHF']\nscores = [eval_results.loc['baseline', 'persona_consistency'], \n          eval_results.loc['sft', 'persona_consistency'],\n          eval_results.loc['rlhf', 'persona_consistency']]\naxes[1].bar(models, scores, color=['gray', 'orange', 'green'])\naxes[1].axhline(y=0.85, color='r', linestyle='--', label='Target (85%)')\naxes[1].set_ylabel('Persona Consistency')\naxes[1].set_title('Model Performance')\naxes[1].legend()\naxes[1].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.savefig('../outputs/project_goals_summary.png', dpi=300)\nwandb.log({'goals_summary': wandb.Image(plt)})\nplt.show()\nprint('\u2705 Visualization saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Chatbot Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(persona_traits, user_message, history=[]):\n    \"\"\"Chat with persona-consistent bot\"\"\"\n    # Format prompt\n    persona_text = ' | '.join(persona_traits)\n    context_str = ' [SEP] '.join(history[-3:]) if history else ''\n    prompt = f'[PERSONA] {persona_text} [DIALOGUE] {context_str} User: {user_message} [RESPONSE]'\n    \n    # Generate\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.9, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n    \n    return response.strip()\n\nprint('\u2705 Chat function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation\npersona = ['I am a software engineer', 'I love hiking', 'I have two dogs']\n\nprint('\ud83e\udd16 CHATBOT DEMO')\nprint(f'Persona: {\", \".join(persona)}')\nprint('=' * 70)\n\nconversation = []\nquestions = [\n    'Hi! What do you like to do in your free time?',\n    'That sounds fun! What kind of dogs do you have?',\n    'Do you take them hiking with you?'\n]\n\npersona_eval = PersonaEvaluator()\n\nfor q in questions:\n    print(f'\\nUser: {q}')\n    response = chat(persona, q, conversation)\n    print(f'Bot: {response}')\n    \n    # Check consistency\n    consistency = persona_eval.calculate_consistency_score(response, persona)\n    print(f'Consistency: {consistency:.1%}')\n    \n    conversation.append(f'User: {q}')\n    conversation.append(f'Bot: {response}')\n\nprint('\\n' + '=' * 70)\nprint('Demo complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\nprint('\\n\u2705 PROJECT COMPLETE!')\nprint('All notebooks executed successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}