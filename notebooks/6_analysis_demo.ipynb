{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis & Interactive Demo\n\n**Final verification of all project goals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft wandb matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../')\n",
    "import torch, json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.model.base import load_base_model, load_tokenizer\n",
    "from src.utils.metrics import MetricsTracker\n",
    "from src.eval.persona import PersonaEvaluator\n",
    "\n",
    "# Optional wandb - no API key required\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(project='persona-chatbot-rlhf', name='analysis-demo', mode='disabled')  # offline mode\n",
    "    USE_WANDB = True\n",
    "except:\n",
    "    USE_WANDB = False\n",
    "    class wandb:\n",
    "        @staticmethod\n",
    "        def log(*args, **kwargs): pass\n",
    "        @staticmethod\n",
    "        def finish(): pass\n",
    "        class Image:\n",
    "            def __init__(self, *args, **kwargs): pass\n",
    "\n",
    "print(f'W&B: {\"enabled (offline)\" if USE_WANDB else \"disabled\"}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Best Model (RLHF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_base_model({'name': '../models/rlhf/checkpoint-final', 'device_map': 'auto'})\n",
    "tokenizer = load_tokenizer({'name': '../models/rlhf/checkpoint-final'})\n",
    "model.eval()\n",
    "print('\u2705 RLHF model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify All Project Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training summary\n",
    "with open('../models/sft/summary.json', 'r') as f:\n",
    "    sft_summary = json.load(f)\n",
    "\n",
    "# Load evaluation results\n",
    "import pandas as pd\n",
    "eval_results = pd.read_csv('../outputs/evaluation_results.csv', index_col=0)\n",
    "\n",
    "print('\ud83c\udfaf PROJECT GOALS VERIFICATION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Goal 1: Cost Reduction (75-80%)\n",
    "cost_savings = sft_summary['savings_cost%']\n",
    "cost_met = cost_savings >= 75\n",
    "print(f'1. Cost Reduction:')\n",
    "print(f'   Target: 75-80%')\n",
    "print(f'   Achieved: {cost_savings:.1f}%')\n",
    "print(f'   Status: {\"\u2705 ACHIEVED\" if cost_met else \"\u274c NOT MET\"}')\n",
    "\n",
    "# Goal 2: Time Reduction (60-70%)\n",
    "time_savings = sft_summary['savings_time%']\n",
    "time_met = time_savings >= 60\n",
    "print(f'\\n2. Time Reduction:')\n",
    "print(f'   Target: 60-70%')\n",
    "print(f'   Achieved: {time_savings:.1f}%')\n",
    "print(f'   Status: {\"\u2705 ACHIEVED\" if time_met else \"\u274c NOT MET\"}')\n",
    "\n",
    "# Goal 3: Persona Consistency (85%+)\n",
    "persona_consistency = eval_results.loc['rlhf', 'persona_consistency']\n",
    "persona_met = persona_consistency >= 0.85\n",
    "print(f'\\n3. Persona Consistency:')\n",
    "print(f'   Target: 85%+')\n",
    "print(f'   Achieved: {persona_consistency:.1%}')\n",
    "print(f'   Status: {\"\u2705 ACHIEVED\" if persona_met else \"\u274c NOT MET\"}')\n",
    "\n",
    "# Overall\n",
    "all_met = cost_met and time_met and persona_met\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print(f'OVERALL: {\"\u2705 ALL GOALS ACHIEVED\" if all_met else \"\u26a0\ufe0f SOME GOALS NOT MET\"}')\n",
    "print(f'{\"=\" * 60}')\n",
    "\n",
    "wandb.log({'goal_cost_met': cost_met, 'goal_time_met': time_met, 'goal_persona_met': persona_met, 'all_goals_met': all_met})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cost-Benefit Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize savings\n",
    "import os\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Cost & Time Savings\n",
    "savings_data = {'Cost Savings': cost_savings, 'Time Savings': time_savings}\n",
    "axes[0].bar(savings_data.keys(), savings_data.values(), color=['green', 'blue'])\n",
    "axes[0].axhline(y=75, color='r', linestyle='--', label='Cost Target (75%)')\n",
    "axes[0].axhline(y=60, color='orange', linestyle='--', label='Time Target (60%)')\n",
    "axes[0].set_ylabel('Savings (%)')\n",
    "axes[0].set_title('LoRA Efficiency Gains')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "# Model Comparison\n",
    "models = ['Baseline', 'SFT', 'RLHF']\n",
    "scores = [eval_results.loc['baseline', 'persona_consistency'], \n",
    "          eval_results.loc['sft', 'persona_consistency'],\n",
    "          eval_results.loc['rlhf', 'persona_consistency']]\n",
    "axes[1].bar(models, scores, color=['gray', 'orange', 'green'])\n",
    "axes[1].axhline(y=0.85, color='r', linestyle='--', label='Target (85%)')\n",
    "axes[1].set_ylabel('Persona Consistency')\n",
    "axes[1].set_title('Model Performance')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/project_goals_summary.png', dpi=300)\n",
    "if USE_WANDB:\n",
    "    wandb.log({'goals_summary': wandb.Image(plt)})\n",
    "plt.show()\n",
    "print('\u2705 Visualization saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Chatbot Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(persona_traits, user_message, history=[]):\n",
    "    \"\"\"Chat with persona-consistent bot\"\"\"\n",
    "    # Format prompt\n",
    "    persona_text = ' | '.join(persona_traits)\n",
    "    context_str = ' [SEP] '.join(history[-3:]) if history else ''\n",
    "    prompt = f'[PERSONA] {persona_text} [DIALOGUE] {context_str} User: {user_message} [RESPONSE]'\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.9, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print('\u2705 Chat function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation\n",
    "persona = ['I am a software engineer', 'I love hiking', 'I have two dogs']\n",
    "\n",
    "print('\ud83e\udd16 CHATBOT DEMO')\n",
    "print(f'Persona: {\", \".join(persona)}')\n",
    "print('=' * 70)\n",
    "\n",
    "conversation = []\n",
    "questions = [\n",
    "    'Hi! What do you like to do in your free time?',\n",
    "    'That sounds fun! What kind of dogs do you have?',\n",
    "    'Do you take them hiking with you?'\n",
    "]\n",
    "\n",
    "persona_eval = PersonaEvaluator()\n",
    "\n",
    "for q in questions:\n",
    "    print(f'\\nUser: {q}')\n",
    "    response = chat(persona, q, conversation)\n",
    "    print(f'Bot: {response}')\n",
    "    \n",
    "    # Check consistency\n",
    "    consistency = persona_eval.calculate_consistency_score(response, persona)\n",
    "    print(f'Consistency: {consistency:.1%}')\n",
    "    \n",
    "    conversation.append(f'User: {q}')\n",
    "    conversation.append(f'Bot: {response}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print('Demo complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "print('\\n\u2705 PROJECT COMPLETE!')\n",
    "print('All notebooks executed successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}