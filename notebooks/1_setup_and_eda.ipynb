{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona-Consistent Chatbot: Setup & EDA\n",
    "\n",
    "## Environment Setup and Exploratory Data Analysis\n",
    "\n",
    "This notebook covers:\n",
    "- Environment setup and package installation\n",
    "- Dataset downloading and exploration\n",
    "- Data quality analysis\n",
    "- Persona trait analysis\n",
    "- Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft trl accelerate wandb\n",
    "!pip install -q rouge-score sacrebleu evaluate\n",
    "!pip install -q matplotlib seaborn pandas numpy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PersonaChat dataset\n",
    "print(\"Loading PersonaChat dataset...\")\n",
    "personachat = load_dataset(\"google/Synthetic-Persona-Chat\")\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "for split, data in personachat.items():\n",
    "    print(f\"{split}: {len(data)} examples\")\n",
    "\n",
    "# Load Blended Skill Talk\n",
    "print(\"\\nLoading Blended Skill Talk dataset...\")\n",
    "bst = load_dataset(\"blended_skill_talk\")\n",
    "for split, data in bst.items():\n",
    "    print(f\"{split}: {len(data)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper functions for flexible dataset field handling\ndef get_persona(example):\n    \"\"\"Get persona from example (works with multiple dataset formats)\"\"\"\n    # Google Synthetic-Persona-Chat uses 'user_1_persona' and 'user_2_persona' (with underscores)\n    for field in ['user_1_persona', 'user_2_persona', 'personality', 'persona', 'personas', 'user_persona', 'user 1 personas', 'user 2 personas']:\n        if field in example and example[field]:\n            return example[field] if isinstance(example[field], list) else [example[field]]\n    return []\n\ndef get_conversation(example):\n    \"\"\"Get conversation from example (works with multiple dataset formats)\"\"\"\n    # Google Synthetic-Persona-Chat uses 'utterances' field\n    # Try all possible field names (ordered by likelihood)\n    for field in ['utterances', 'history', 'conversation', 'dialogue', 'messages', 'Best Generated Conversation']:\n        if field in example and example[field]:\n            value = example[field]\n            if isinstance(value, list):\n                return value\n            elif isinstance(value, str):\n                # Split by newlines\n                return [line.strip() for line in value.split('\\n') if line.strip()]\n    return []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine PersonaChat structure\n",
    "print(\"PersonaChat example:\")\n",
    "example = personachat['train'][0]\n",
    "print(f\"Personality traits: {get_persona(example)}\")\n",
    "print(f\"History: {get_conversation(example)}\")\n",
    "print(f\"\\nFull example keys: {list(example.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze persona traits distribution\n",
    "all_traits = []\n",
    "for example in personachat['train']:\n",
    "    all_traits.extend(get_persona(example))\n",
    "\n",
    "print(f\"Total unique persona traits: {len(set(all_traits))}\")\n",
    "print(f\"Average traits per persona: {np.mean([len(get_persona(ex)) for ex in personachat['train']]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trait frequency\n",
    "from collections import Counter\n",
    "trait_counts = Counter(all_traits)\n",
    "top_traits = trait_counts.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "traits, counts = zip(*top_traits)\n",
    "plt.bar(range(len(traits)), counts)\n",
    "plt.xticks(range(len(traits)), traits, rotation=45, ha='right')\n",
    "plt.title('Top 20 Persona Traits')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze conversation lengths\n",
    "conversation_lengths = [len(get_conversation(ex)) for ex in personachat['train']]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(conversation_lengths, bins=30, alpha=0.7)\n",
    "plt.title('Conversation Length Distribution')\n",
    "plt.xlabel('Number of turns')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "response_lengths = [len(turn.split()) for ex in personachat['train'] for turn in get_conversation(ex)]\n",
    "plt.hist(response_lengths, bins=30, alpha=0.7, color='orange')\n",
    "plt.title('Response Length Distribution')\n",
    "plt.xlabel('Words per response')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average conversation length: {np.mean(conversation_lengths):.1f} turns\")\n",
    "print(f\"Average response length: {np.mean(response_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "empty_personas = sum(1 for ex in personachat['train'] if not get_persona(ex))\n",
    "short_conversations = sum(1 for ex in personachat['train'] if len(get_conversation(ex)) < 2)\n",
    "\n",
    "print(\"Data Quality Report:\")\n",
    "print(f\"Examples with empty personas: {empty_personas} ({empty_personas/len(personachat['train'])*100:.1f}%)\")\n",
    "print(f\"Short conversations (<2 turns): {short_conversations} ({short_conversations/len(personachat['train'])*100:.1f}%)\")\n",
    "print(f\"Total training examples: {len(personachat['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample conversations\n",
    "print(\"Sample Conversations from PersonaChat:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    example = personachat['train'][i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Persona: {', '.join(get_persona(example))}\")\n",
    "    print(\"Conversation:\")\n",
    "    for j, turn in enumerate(get_conversation(example)):\n",
    "        speaker = \"User\" if j % 2 == 0 else \"Bot\"\n",
    "        print(f\"  {speaker}: {turn}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all components are working\n",
    "print(\"Setup Verification:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    from src.data.loader import DatasetLoader\n",
    "    from src.utils.config import load_config\n",
    "    print(\"✅ Source code imports working\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "\n",
    "# Test config loading\n",
    "try:\n",
    "    config = load_config('../config/model.yaml')\n",
    "    print(\"✅ Configuration loading working\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Config error: {e}\")\n",
    "\n",
    "# Test dataset loading\n",
    "try:\n",
    "    loader = DatasetLoader()\n",
    "    data = loader.load_personachat()\n",
    "    print(f\"✅ Dataset loading working ({len(data)} examples)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Dataset error: {e}\")\n",
    "\n",
    "print(\"\\nSetup completed successfully! ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "- ✅ Set up the environment and installed dependencies\n",
    "- ✅ Loaded and explored the PersonaChat and BST datasets\n",
    "- ✅ Analyzed persona traits and conversation patterns\n",
    "- ✅ Verified data quality and sample conversations\n",
    "- ✅ Confirmed all components are working correctly\n",
    "\n",
    "Next: Proceed to `2_baseline_testing.ipynb` to test base models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}